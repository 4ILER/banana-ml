{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative Adversarial Networks\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/torch/torch.github.io/master/blog/_posts/images/model.png\" width=320px height=240px>\n",
    "\n",
    "Начнем с оочень простой модельки, которая генирирует значения из нормального распределения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sample_noise(batch_size):\n",
    "    \"\"\" Uniform noise of shape [batch_size, 1] in range [0, 1]\"\"\"\n",
    "    return torch.rand(batch_size, 1)\n",
    "\n",
    "def sample_real_data(batch_size):\n",
    "    \"\"\" Normal noise of shape [batch_size, 1], mu=5, std=1.5 \"\"\"\n",
    "    return torch.randn(batch_size, 1) * 1.5 + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator converts 1d noise into 1d data\n",
    "gen = nn.Sequential(nn.Linear(1, 16),\n",
    "                    nn.ELU(),\n",
    "                    \n",
    "                    nn.Linear(16, 1))\n",
    "gen_opt = torch.optim.SGD(gen.parameters(), lr=1e-3)\n",
    "\n",
    "# Discriminator converts 1d data into two logits (0th for real, 1st for fake). \n",
    "# It is deliberately made stronger than generator to make sure disc is slightly \"ahead in the game\".\n",
    "disc = nn.Sequential(nn.Linear(1, 64),\n",
    "                     nn.ELU(),\n",
    "                     nn.Linear(64, 2))\n",
    "disc_opt = torch.optim.SGD(disc.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define 0-th output of discriminator as \"is_fake\" output and 1-st as \"is_real\"\n",
    "IS_FAKE, IS_REAL = 0, 1\n",
    "\n",
    "def train_disc(batch_size):\n",
    "    \"\"\" trains discriminator for one step \"\"\"\n",
    "    \n",
    "    # compute logp(real | x)\n",
    "    real_data = sample_real_data(batch_size)\n",
    "    logp_real_is_real = F.log_softmax(disc(real_data), dim=1)[:, IS_REAL]\n",
    "    \n",
    "    # compute logp(fake | G(z)). We detach to avoid computing gradinents through G(z)\n",
    "    noise = <sample noise>\n",
    "\n",
    "    logp_gen_is_fake =  <generate data given noise>\n",
    "\n",
    "    disc_loss = <compute loss>\n",
    "    \n",
    "    # sgd step. We zero_grad first to clear any gradients left from generator training\n",
    "    disc_opt.zero_grad()\n",
    "    disc_loss.backward()\n",
    "    disc_opt.step()\n",
    "    return disc_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gen(batch_size):\n",
    "    \"\"\" trains generator for one step \"\"\"\n",
    "        \n",
    "    # compute logp(fake | G(z)).\n",
    "    noise = <sample noise>\n",
    "    gen_data = <generate data given noise>\n",
    "    \n",
    "    logp_gen_is_real = <compute logp gen is REAL>\n",
    "\n",
    "    gen_loss = <generator loss>\n",
    "\n",
    "    gen_opt.zero_grad()\n",
    "    gen_loss.backward()\n",
    "    gen_opt.step()\n",
    "    return gen_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DISC_STEPS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "for i in range(100000):\n",
    "\n",
    "    for _ in range(NUM_DISC_STEPS):\n",
    "        train_disc(128)\n",
    "    \n",
    "    train_gen(128)\n",
    "    \n",
    "    if i % 250 == 0:\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=[14, 6])\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Data distributions\")\n",
    "        plt.hist(gen(sample_noise(1000)).data.numpy()[:, 0], range=[0, 10], alpha=0.5, density=True, label='gen')\n",
    "        plt.hist(sample_real_data(1000).data.numpy()[:,0], range=[0, 10], alpha=0.5, density=True, label='real')\n",
    "        \n",
    "        x = np.linspace(0,10, dtype='float32')\n",
    "        disc_preal = F.softmax(disc(torch.from_numpy(x[:, None])))[:, 1]\n",
    "        plt.plot(x, disc_preal.data.numpy(), label='disc P(real)')\n",
    "        plt.legend()\n",
    "        \n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Discriminator readout on real vs gen\")\n",
    "        plt.hist(F.softmax(disc(gen(sample_noise(100))))[:, 1].data.numpy(),\n",
    "                 range=[0, 1], alpha=0.5, label='D(G(z))')\n",
    "        plt.hist(F.softmax(disc(sample_real_data(100)))[:, 1].data.numpy(),\n",
    "                 range=[0, 1], alpha=0.5, label='D(x)')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
